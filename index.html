<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models</title>
  <link rel="icon" href="static/images/ai.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://sites.google.com/view/mcye" style="text-decoration: none; color: inherit;">Muchao Ye<sup style="color:#6fbf73;">1</sup></a>,</span>
                <span class="author-block">
                  <a href="https://wyliu.com/" style="text-decoration: none; color: inherit;">Weiyang Liu<sup style="color:#6fbf73;">2</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://panhe.org/" style="text-decoration: none; color: inherit;">Pan He<sup style="color:#6fbf73;">3</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>The University of Iowa</span>
                    <span class="author-block"><sup>2</sup>Max Planck Institute for Intelligent Systems - TÃ¼bingen</span>
                    <span class="author-block"><sup>3</sup>Auburn University</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2412.01095" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>



                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.01095" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
        <div style="text-align: center;">
            <img src="static/images/fig1.png" alt="VERA Framework" width="80%"/>
        </div>
        <div class="content" style="text-align: left;"> 
          <p> VERA renders frozen vision-language models (VLMs) to describe and reason with learnable guiding questions learned from coarsely labeled data. Compared to existing pipelines that use VLMs for video anomaly detection, VERA do not need extra reasoning modules or additional instruction tuning on extra annotated data.</p>
        </div>
    </div>
  </div>
</section>


  

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


</div>

  
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Training in VERA</h2>
          <div class="content has-text-justified">
            <p>
              VERA aims to learn guiding questions that break down a complex and ambiguous concept (i.e., what is an "anomaly") into a set of identifiable anomalous patterns to unlock reasoning capabilities within frozen VLMs for VAD. Since those patterns vary among datasets, using manually designed descriptions is ineffective for generalization. The key idea of VERA is using a general verbalized learning framework shown in the figure below to generate the desired guiding questions automatically.
            </p>
            <div class="content has-text-centered">
              <img src="static/images/fig2.png" alt="VERA Training" class="center" style="width: 80%; height: auto;">
              <p> VERA regards the guiding questions for VAD as learnable parameters and optimizes them by the feedback from the optimizer based on the performance of the learner in the learning task. </p>
            </div>
            <p>
            As shown in the figure above, VERA inherits the idea of verbalized machine learning in training:  optimizing language-based parameters by the iterative verbal communication between a learner agent and an optimizer agent, rather than by numerical optimization algorithms. The learner will first make predictions on a batch of training data (sampled video frames and coarsely labeled annotations), and the optimizer will judge the performance of the learner in the designed binary classification task and provide feedback on the choice of guiding questions. They are modeled by the same VLMs in training with different instructions detailed in the template.
            </p>
          </div>
        </div>
      </div>
    </div>



      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Inference in VERA</h2>
          <div class="content has-text-justified">
            <p>
              Given a set of learned guiding questions, VERA yields fine-grained anomaly scores for each frame in a test video via a coarse-to-fine process shown in the figure below. Step 1 is to ask the VLM to provide an initial anomaly score for each segment. Step 2 is to utilize the scene context and compute an ensemble score for each segment after considering its similar scenes. Step 3 is to generate a frame-level anomaly score for each frame by fusing temporal context with segment-level Gaussian smoothing and frame-level position weighting.   
            </p>
            <div class="content has-text-centered">
              <img src="static/images/fig3.png" alt="VERA Training" class="center" style="width: 50%; height: auto;">
              <p> After finding the optimal guiding questions, VERA computes anomaly scores in three steps. </p>
            </div>
            <p>
            When using the learner template embedded with the learned guiding questions to compute the anomaly score, we ask the VLM to "provide an explanation in one sentence" when reasoning, and VLM will explain the anomaly score it assigns afterward based on the learned guiding questions. The instruction-following property of VLMs help us attain explainable VAD prediction.
            </p>
          </div>
        </div>
      </div>
    </div>

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>



      
        <div class="content has-text-justified">
            <p>
              We take an abnormal video (RoadAccidents127_x264) from the UCF-Crime dataset to demonstrate the explainable VAD provided by a frozen VLM (InternVL2-8B) achieved by using the learned guiding questions via VERA. The complete video is as follows: 
            </p>

            <div style="text-align: center;">
              <video class="media" controls style="display: block; margin-left: auto; margin-right: auto;">
                <source src="static/videos/RoadAccidents127_x264.mp4" type="video/mp4">
              </video>
            </div>
          
              The questions learned by the VERA are as follows: 
            </p>

          <ul>
            <li>Are there any people in the video who are not in their typical positions or engaging in activities that are not consistent with their usual behavior?</li>
            <li>Are there any vehicles in the video that are not in their typical positions or being used in a way that is not consistent with their usual function?</li>
            <li>Are there any objects in the video that are not in their typical positions or being used in a way that is not consistent with their usual function?</li>
            <li>Is there any visible damage or unusual movement in the video that indicates an anomaly?</li>
            <li>Are there any unusual sounds or noises in the video that suggest an anomaly?</li>
          </ul>

          <p>
            we select 6 representative scenes in the this video and show the corresponding explanation provided by the frozen VLM as follows. The main anomaly in this video is a traffic accident where a truck crashes into a train from Frame 2160 to Frame 2299, which corresponds to the 5th scene below. In particular, the learned question "Is there any visible damage or unusual movement in the video that indicates an anomaly?" makes the frozen VLM find a good way to express what it sees in the 5th scene and understand this is an anomaly because the crash is unusual and dangerous. The other scenes are also well explained by the frozen VLM under the learned guiding questions. Thus, this verifies that the learned guiding questions can successfully trigger reasonable explanations in the adopted frozen VLM for VAD.
          </p> 
            <!-- GIFs in a 2x3 grid -->
            <div class="gif-container" style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px;">
                <!-- GIF 1 -->
                <div class="gif-item" style="text-align: left;">
                <div style="text-align: center;">
                    <img src="static/images/scene_1.gif" alt="Scene 1" style="width: 100%; max-width: 300px; height: auto; border-radius: 8px;">
                </div>
                    <p style="margin-top: 8px;"><strong>VERA's Explanation for Scene 1</strong>: The video shows a red truck driving on a road with other vehicles and pedestrians. The truck appears to be in its typical position and engaging in a consistent activity. Therefore, there is no anomaly in the video.</p>
              
                </div>


                <!-- GIF 2 -->

                <div class="gif-item" style="text-align: left;">
                <div style="text-align: center;">
                    <img src="static/images/scene_2.gif" alt="Scene 2" style="width: 100%; max-width: 300px; height: auto; border-radius: 8px;">
                </div>
                    <p style="margin-top: 8px;"><strong>VERA's Explanation for Scene 2</strong>: The video shows people crossing the street, but one person is not in a typical position and is running, which could be an anomaly. Based on the analysis, there is an anomaly in the video.</p>
              
                </div>

                <!-- GIF 3 -->
                <div class="gif-item" style="text-align: left;">
                <div style="text-align: center;">
                    <img src="static/images/scene_3.gif" alt="Scene 3" style="width: 100%; max-width: 300px; height: auto; border-radius: 8px;">
                </div>
                    <p style="margin-top: 8px;"><strong>VERA's Explanation for Scene 3</strong>: There are no people in the video, and the vehicles and objects are in their typical positions and functions. There is no visible damage or unusual movement, and no unusual sounds or noises are present. Therefore, there is no anomaly in the video.</p>
              
                </div>

                <!-- GIF 4 -->
      
                <div class="gif-item" style="text-align: left;">
                <div style="text-align: center;">
                    <img src="static/images/scene_4.gif" alt="Scene 4" style="width: 100%; max-width: 300px; height: auto; border-radius: 8px;">
                </div>
                    <p style="margin-top: 8px;"><strong>VERA's Explanation for Scene 4</strong>: There are no people in the video, and the vehicles and objects are in their typical positions and functions. There is no visible damage or unusual movement, and no unusual sounds or noises are present. Therefore, there is no anomaly in the video.</p>
              
                </div>

                <!-- GIF 5 -->

                <div class="gif-item" style="text-align: left;">
                <div style="text-align: center;">
                    <img src="static/images/scene_5.gif" alt="Scene 5" style="width: 100%; max-width: 300px; height: auto; border-radius: 8px;">
                </div>
                    <p style="margin-top: 8px;"><strong>VERA's Explanation for Scene 5</strong>: The video shows a red truck colliding with a train at a crossing. This is an unusual and dangerous situation, indicating an anomaly.</p>
              
                </div>
              
                <!-- GIF 6 -->
                <div class="gif-item" style="text-align: left;">
                <div style="text-align: center;">
                  <img src="static/images/scene_6.gif" alt="Scene 6" style="width: 100%; max-width: 300px; height: auto; border-radius: 8px;">
                </div>
                <p style="margin-top: 8px;"> <strong>VERA's Explanation for Scene 6</strong>: The video shows a train on the tracks, and there are no people or vehicles in unusual positions or engaging in activities that are not consistent with their usual behavior. Based on the analysis, there is no anomaly in the video.</p>                </div>
              
                </div>


                 <div class="content has-text-centered">
              <img src="static/images/fig12.png" alt="VERA Anomaly Scoring" class="center" style="width: 30%; height: auto;">
            </div>
            <p> We include the anomaly scores generated by VERA for the entire video video here. We can see most frames are assigned to zero except when someone crosses the road at an unusual speed (the 2nd scene above) and the truck-train crash happens (the 5th scene above). This fluctuation is aligned with the ground truth annotation and common sense about an anomaly, which shows that the anomaly scoring proposed in VERA is reasonable.</p>
          

    

          
        </div>
    </div>
</div>




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{ye2024veraexplainablevideoanomaly,
      title={VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models}, 
      author={Muchao Ye and Weiyang Liu and Pan He},
      year={2024},
      eprint={2412.01095},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.01095}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
